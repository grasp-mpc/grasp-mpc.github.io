<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="MPC, Grasping, Robot Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://junjungoal.github.io/">Jun Yamada</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="http://adithyamurali.com/">Adithyavairavan Murali</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://ai.stanford.edu/~amandlek/">Ajay Mandlekar</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://clemense.github.io/">Clemens Eppner</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://eng.ox.ac.uk/people/ingmar-posner/">Ingmar Posner</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Oxford,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2509.06201"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.06201"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
        <div class="columns is-vcentered is-centered">
            <img src="./static/images/overview.png"
                 class="interpolation-image"
                 alt="Grasp-MPC Overview" width='100%'/>
        </div>
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Abstract</h2> -->
        <div class="content has-text-justified">
          <p>Grasping remains a significant challenge in robotics, particularly in unstructured environments with diverse and novel objects.
            Open-loop grasping methods, while effective in controlled settings, often struggle in dynamic or cluttered environments due to their inability to adapt to object pose changes or recover from grasp prediction errors.
            In contrast, state-of-the-art closed-loop methods address some of these challenges but are often restricted to simplified settings, lack robustness in safely executing grasps in cluttered scenes, and exhibit poor generalization to novel objects.
          To address these, we propose <em>Grasp-MPC</em>, a closed-loop 6-DoF vision-based grasping policy designed for robust and reactive grasping for novel objects in cluttered environments.
          <em>Grasp-MPC</em> incorporates a value function, trained on visual observations from a large-scale synthetic dataset of 2 million grasp trajectories, including both successful and failed attempts, into a MPC framework.
          The value function serves as a cost term, combined with collision and minimum jerk costs, enabling real-time, safe, and adaptive grasp execution.
          To initiate each grasp attempt, <em>Grasp-MPC</em> leverages an off-the-shelf grasp prediction model to move the gripper to a pre-grasp pose, from which it executes closed-loop control for grasping.
          As a closed-loop policy, <em>Grasp-MPC</em> can dynamically react to changes in object pose, a capability validated in real-world experiments.
          <em>Grasp-MPC</em> is extensively evaluated on FetchBench and real-world settings, including tabletop, cluttered tabletop, and shelf scenarios, where it significantly outperforms baselines in grasp success and generalizes well to novel objects.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/Ae7FubIR5P0?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<!-- <section class="section"> -->
<!--   <div class="container is-max-desktop"> -->
<!--     <div class="columns is-centered"> -->
<!--       <div class="column is-full-width"> -->
<!--         <h2 class="title is-3">Overview</h2> -->
<!--  -->
<!--         &#60;&#33;&#45;&#45; Interpolating. &#45;&#45;&#62; -->
<!--         <div class="columns is-vcentered is-centered"> -->
<!--             <img src="./static/images/overview.png" -->
<!--                  class="interpolation-image" -->
<!--                  alt="Grasp-MPC Overview" width='100%'/> -->
<!--         </div> -->
<!--         <div class="content has-text-justified"> -->
<!--           <p> -->
<!--             A large-scale synthetic grasp trajectory dataset is generated in simulation using a motion -->
<!--             planner, collecting only trajectories between pre-grasp and ground-truth grasp poses for around 8K Objaverse objects. Value -->
<!--             functions are trained using a sparse cost label given the target objectâ€™s point cloud and the end-effector pose relative to the -->
<!--             point cloud center. The learned value function is used as a cost in an MPC framework, enabling robust and safe grasping -->
<!--             of novel objects in cluttered environments. -->
<!--           </p> -->
<!--         </div> -->
<!--         </div> -->
<!--         <br/> -->
<!--       </div> -->
<!--   </div> -->
<!-- </section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    <h3 class="title is-4">Tabletop Clutter</h3>
    <div class="columns is-centered">      
      <!-- Visual Effects. -->

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/red_cup_grasp_mpc_pose1_trial1_success.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/spam_grasp_mpc_pose3_trial1_success.mov"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <h3 class="title is-4">Shelf Clutter</h3>
    <div class="columns is-centered">      
      <!-- Visual Effects. -->

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/light_grasp_mpc_pose2_trial1_success.m4v"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/sprey_grasp_mpc_pose3_trial1_success.mov"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <h3 class="title is-4">Moving Objects</h3>
    <div class="columns is-centered">      
      <!-- Visual Effects. -->

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/peach_pose2_success.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/sponge_pose6_success.mov"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <h3 class="title is-4">Sequential Object Grasping</h3>
    <div class="columns is-centered">      
      <!-- Visual Effects. -->

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo1_x8.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/demo2_x8.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Jun Yamada and Adithyavairavan Murali and Ajay Mandlekar and Clemens Eppner and Ingmar Posner and Balakumar Sundaralingam},
  title     = {Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control},
  journal={arXiv preprint arXiv:2509.06201},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is made from the template from  <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>        
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
